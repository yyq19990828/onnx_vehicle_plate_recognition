# 在 TensorRT 中使用动态形状

## 简介

*注意：此示例适用于 TensorRT 8.0 或更高版本。*
    *旧版本可能需要对示例代码进行少量修改。*

为了在 TensorRT 中使用动态输入形状，我们必须在构建引擎时指定一个或多个可能的形状范围。
TensorRT 优化配置文件提供了实现此目的的方法。

使用 TensorRT API，该过程包括两个步骤：

1. 在引擎构建期间，指定一个或多个优化配置文件。
    一个优化配置文件包括每个输入的 3 个形状：
    - `min`：配置文件应工作的最小形状。
    - `opt`：TensorRT 应优化的形状。
        通常，您希望这对应于最常用的形状。
    - `max`：配置文件应工作的最大形状。

2. 在推理期间，在执行上下文中设置输入形状，然后
    使用 `IOutputAllocator` API 提供回调以分配足够的
    设备内存用于输出。

Polygraphy 可以简化这两个步骤，并帮助您避免常见的陷阱：

1. 它提供了一个 `Profile` 抽象，它是一个 `OrderedDict`，
    可以转换为 TensorRT `IOptimizationProfile` 并包括一些实用函数：
    - `fill_defaults`：根据网络使用默认形状填充配置文件。
    - `to_trt`：使用此 `Profile` 中的形状创建 TensorRT `IOptimizationProfile`。

    更重要的是，`Profile` 会自动处理复杂性，例如
    形状张量与非形状张量输入之间的区别 - 您不需要
    自己担心这种区别。

2. `TrtRunner` 将自动处理模型中的动态形状。
    与 `Profile` 中一样，形状张量和非形状张量输入之间的区别
    是自动处理的。

    此外，运行器仅在需要时更新上下文绑定形状，
    因为更改形状会产生少量开销。输出设备缓冲区仅
    在当前大小小于上下文输出时才会调整大小，从而避免
    不必要的重新分配。


### 设定场景

为了本示例的目的，我们假设一个假设场景：

我们正在使用图像分类模型运行推理工作负载。

通常，我们在在线场景中使用此模型 - 即我们希望获得尽可能低的
延迟，因此我们将一次处理一张图像。
对于这种情况，假设 `batch_size` 为 `[1]`。

但是，如果用户过多，则需要采用动态批处理，以便
我们的吞吐量不会受到影响。我们的批量大小范围仍然很小，以
保持可接受的延迟。我们最常用的批量大小是 4。
对于这种情况，假设 `batch_size` 在 `[1, 32]` 范围内。

在更罕见的情况下，我们需要离线处理大量数据。在这种情况下，
我们使用非常大的批量大小来提高吞吐量。
对于这种情况，假设 `batch_size` 为 `[128]`。

### 性能注意事项

在实现我们的推理管道时，我们需要考虑一些权衡：

- 具有大范围的配置文件在整个范围内的性能不如
    多个每个都具有较小范围的配置文件。
- 在配置文件中切换形状的成本很小但非零。
- 在上下文中切换配置文件的成本比在配置文件中切换形状的成本更大。
    - 我们可以通过为每个配置文件创建单独的执行上下文
        并在运行时选择适当的上下文来避免切换配置文件的成本。
        但是，请记住，每个上下文都需要一些额外的内存。


### 一个可能的解决方案

假设图像大小为 `(3, 28, 28)`，我们将创建三个独立的
优化配置文件，并为每个配置文件创建一个单独的上下文：

1. 对于低延迟情况：
    `min=(1, 3, 28, 28), opt=(1, 3, 28, 28), max=(1, 3, 28, 28)`

2. 对于动态批处理情况：
    `min=(1, 3, 28, 28), opt=(4, 3, 28, 28), max=(32, 3, 28, 28)`

    请注意，我们使用批量大小为 `4` 的 `opt`，因为这是最常见的情况。

3. 对于离线情况：
    `min=(128, 3, 28, 28), opt=(128, 3, 28, 28), max=(128, 3, 28, 28)`

对于每个上下文，我们将创建一个相应的 `TrtRunner`。如果我们确保
我们拥有引擎和上下文（通过不通过延迟加载器提供它们），那么
激活运行器的成本应该很小 - 它只需要分配
输入和输出缓冲区。因此，我们将能够快速按需激活运行器。


## 运行示例

1.  安装先决条件
    *   确保已安装 TensorRT
    *   使用 `python3 -m pip install -r requirements.txt` 安装其他依赖项

2.  运行示例：

    ```bash
    python3 example.py
    ```

3.  **[可选]** 检查生成的引擎：

    ```bash
    polygraphy inspect model dynamic_identity.engine
    ```

## 更多阅读

有关在 TensorRT 中使用动态形状的更多信息，请参阅
[开发人员指南](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#work_dynamic_shapes)
